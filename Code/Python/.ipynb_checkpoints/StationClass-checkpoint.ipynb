{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from SolarDefs import *\n",
      "from timeit import timeit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_dir = \"/home/rox/Code/CS6601P2/Data/\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from SolarDirectories import data_dir # = '/media/Data/Project Data/Solar Energy/' \n",
      "# Set to your data directory assumes all data is in there - no nesting\n",
      "files_to_use = 'all' # Choices for files_to_use: the string all, or a list of strings corresponding to the unique part of a GEFS filename\n",
      "submit_name = 'submission_mod_whours.csv'\n",
      "args = { 'files_to_use': files_to_use, 'submit_name': submit_name}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainT, trainX, trainY, GEFS_infos, augmentX =load_Training_Data(data_dir,files_to_use)\n",
      "print 'trainT', trainT.shape\n",
      "print 'trainX', trainX.shape\n",
      "print 'trainY', trainY.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loading training data...\n",
        "loading dswrf_sfc\n",
        "this is the path:  /home/rox/Code/CS6601P2/Data/train/dswrf_sfc_latlon_subset_19940101_20071231.nc\n",
        "loading"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " dlwrf_sfc\n",
        "this is the path:  /home/rox/Code/CS6601P2/Data/train/dlwrf_sfc_latlon_subset_19940101_20071231.nc\n",
        "loading"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " uswrf_sfc\n",
        "this is the path:  /home/rox/Code/CS6601P2/Data/train/uswrf_sfc_latlon_subset_19940101_20071231.nc\n",
        "loading"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ulwrf_sfc\n",
        "this is the path:  /home/rox/Code/CS6601P2/Data/train/ulwrf_sfc_latlon_subset_19940101_20071231.nc\n",
        "loading"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " ulwrf_tatm\n",
        "this is the path:  /home/rox/Code/CS6601P2/Data/train/ulwrf_tatm_latlon_subset_19940101_20071231.nc\n",
        "loading"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " pwat_eatm\n",
        "this is the path:  /home/rox/Code/CS6601P2/Data/train/pwat_eatm_latlon_subset_19940101_20071231.nc\n",
        "loading"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " tcdc_eatm\n",
        "this is the path:  /home/rox/Code/CS6601P2/Data/train/tcdc_eatm_latlon_subset_19940101_20071231.nc\n",
        "loading"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " apcp_sfc\n",
        "this is the path:  /home/rox/Code/CS6601P2/Data/train/apcp_sfc_latlon_subset_19940101_20071231.nc\n",
        "loading"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " pres_msl\n",
        "this is the path:  /home/rox/Code/CS6601P2/Data/train/pres_msl_latlon_subset_19940101_20071231.nc\n",
        "loading"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " spfh_2m\n",
        "this is the path:  /home/rox/Code/CS6601P2/Data/train/spfh_2m_latlon_subset_19940101_20071231.nc\n",
        "loading"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " tcolc_eatm\n",
        "this is the path:  /home/rox/Code/CS6601P2/Data/train/tcolc_eatm_latlon_subset_19940101_20071231.nc\n",
        "loading"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " tmax_2m\n",
        "this is the path:  /home/rox/Code/CS6601P2/Data/train/tmax_2m_latlon_subset_19940101_20071231.nc\n",
        "loading"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " tmin_2m\n",
        "this is the path:  /home/rox/Code/CS6601P2/Data/train/tmin_2m_latlon_subset_19940101_20071231.nc\n",
        "loading"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " tmp_2m\n",
        "this is the path:  /home/rox/Code/CS6601P2/Data/train/tmp_2m_latlon_subset_19940101_20071231.nc\n",
        "loading"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " tmp_sfc\n",
        "this is the path:  /home/rox/Code/CS6601P2/Data/train/tmp_sfc_latlon_subset_19940101_20071231.nc\n",
        "Training data shape"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (5113, 144, 11, 5, 15) (5113, 98)\n",
        "trainT"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (5113,)\n",
        "trainX (5113, 144, 11, 5, 15)\n",
        "trainY (5113, 98)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainDays = dayOfYear(trainT)\n",
      "trainDays = np.expand_dims(trainDays,1)\n",
      "trainCompDays = compexDayOfYear(trainT)\n",
      "trainMonths = monthOfYear(trainT)\n",
      "trainMonths = np.expand_dims(trainMonths,1)\n",
      "trainCompMonths = compexMonthOfYear(trainT)\n",
      "print 'trainDays', trainDays.shape\n",
      "print 'trainCompDays', trainCompDays.shape\n",
      "print 'trainMonths', trainMonths.shape\n",
      "print 'trainCompMonths', trainCompMonths.shape\n",
      "mod_train = np.hstack((trainDays,trainCompDays,trainMonths,trainCompMonths))\n",
      "print 'mod_train', mod_train.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "trainDays (5113, 1)\n",
        "trainCompDays (5113, 2)\n",
        "trainMonths (5113, 1)\n",
        "trainCompMonths (5113, 2)\n",
        "mod_train (5113, 6)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "testT, testX, GEFS_infos, augmentTestX = load_Testing_Data(data_dir,files_to_use)\n",
      "print 'testT', testT.shape\n",
      "print 'testX', testX.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testDays = dayOfYear(testT)\n",
      "testDays = np.expand_dims(testDays,1)\n",
      "testCompDays = compexDayOfYear(testT)\n",
      "testMonths = monthOfYear(testT)\n",
      "testMonths = np.expand_dims(testMonths,1)\n",
      "testCompMonths = compexMonthOfYear(testT)\n",
      "print 'testDays', testDays.shape\n",
      "print 'testCompDays', testCompDays.shape\n",
      "print 'testMonths', testMonths.shape\n",
      "print 'testCompMonths', testCompMonths.shape\n",
      "mod_test = np.hstack((testDays,testCompDays,testMonths,testCompMonths))\n",
      "print 'mod_test', mod_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "station_infos = load_Station_Info(data_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loading station info...\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 1,\n",
      "          'learning_rate': 0.01, 'loss': 'ls'}\n",
      "params_1 = {'loss': 'ls', 'learning_rate': 0.1, 'n_estimators': 100, \n",
      "            'subsample': 1.0, 'min_samples_split': 2, 'min_samples_leaf': 1, \n",
      "            'max_depth': 3, 'init': None, 'random_state': None, \n",
      "            'max_features': None, 'alpha': 0.9, 'verbose': 0, 'learn_rate': None}\n",
      "model = ensemble.GradientBoostingRegressor(**params_1)\n",
      "config = {'nGEFS':4}\n",
      "\n",
      "stations = makeStations(station_infos, GEFS_infos, model, config)\n",
      "\n",
      "#stations = fitStations([stations[0]], trainX, trainY, None, mod_train)\n",
      "#saveModels(stations, data_dir, 'stationModels_1.p')\n",
      "stations = loadModels(stations, data_dir, 'stationModels_mAll_mod.p')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#predictions = predictStations(stations, testX, wModels=None, mod_train=None):\n",
      "#savePickle(predictions, data_dir, 'predictions.p')  \n",
      "predictions = loadPickle(data_dir, 'CV_prediction.p')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def move_operator(weights):\n",
      "    weight_list = []\n",
      "    for i in range(len(weights)):\n",
      "        for j in range(i+1,len(weights)):\n",
      "            temp1 = list(weights)\n",
      "            temp1[i] += 0.01\n",
      "            temp1[j] -= 0.01\n",
      "            weight_list.append(temp1)\n",
      "            temp2 = list(weights)\n",
      "            temp2[i] -= 0.01\n",
      "            temp2[j] += 0.01\n",
      "            weight_list.append(temp2)\n",
      "    #print weight_list\n",
      "    return weight_list\n",
      "\n",
      "def score(truth, predicted, weights):\n",
      "    prediction = (np.array(weights)*np.array(predicted).T).T\n",
      "    final_prediction = np.sum(prediction,axis=0)\n",
      "    return metrics.mean_absolute_error(truth,final_prediction)\n",
      "\n",
      "def random_init(length):\n",
      "    weights = np.random.randint(10, size=length)\n",
      "    sum = np.sum(weights)\n",
      "    if sum == 0:\n",
      "        weights = np.ones(length)/(1.0*length)\n",
      "        sum = np.sum(weights)\n",
      "    weights = weights/(1.0*sum)\n",
      "    #print 'random: ', weights\n",
      "    return weights\n",
      "\n",
      "def hillclimb_reset(weights, truth, prediction, max_evaluations):\n",
      "\n",
      "    best = weights\n",
      "    best_score = score(truth, prediction, best)\n",
      "    \n",
      "    num_evaluations=0\n",
      "    \n",
      "    while num_evaluations < max_evaluations:\n",
      "        remaining_evaluations=max_evaluations-num_evaluations\n",
      "        \n",
      "        evaluated,score,found=hillclimb(weights, truth, prediction, max_evaluations)\n",
      "        \n",
      "        num_evaluations+=evaluated\n",
      "        if score > best_score or best is None:\n",
      "            best_score=score\n",
      "            best=found\n",
      "    \n",
      "    return (num_evaluations,best_score,best)\n",
      "\n",
      "def hillclimb(truth, prediction, max_evaluations):\n",
      "    best = random_init(prediction.shape[0])\n",
      "    best_score = score(truth, prediction, best)\n",
      "    first_score = best_score\n",
      "    \n",
      "    num_evaluations=1\n",
      "    \n",
      "    while num_evaluations < max_evaluations:\n",
      "        # examine moves around our current position\n",
      "        move_made=False\n",
      "        for nextW in move_operator(best):\n",
      "            if num_evaluations >= max_evaluations:\n",
      "                break\n",
      "            \n",
      "            # see if this move is better than the current\n",
      "            next_score = score(truth, prediction, nextW)\n",
      "            num_evaluations+=1\n",
      "            if next_score < best_score:\n",
      "                best=nextW\n",
      "                best_score=next_score\n",
      "                move_made=True\n",
      "                break # depth first search\n",
      "            \n",
      "        if not move_made:\n",
      "            break # we couldn't find a better move \n",
      "                     # (must be at a local maximum)\n",
      "    \n",
      "    return (num_evaluations,best_score,best,first_score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# def applyWeights(X, predictions, segmentWeights, segments, N=12):\n",
      "#     segmentIndexs = []\n",
      "#     for segment in range(N):\n",
      "#         segmentIndexs.append(np.argwhere(segments==(segment+1))[:,0])\n",
      "        \n",
      "#     for i, segment in enumerate(segmentIndexs):\n",
      "#         predictions[:,segment] = (predictions[:,segment].T*np.array(segmentWeights[i])).T\n",
      "#         prediction = np.sum(predictions,0)\n",
      "#     return prediction"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 195
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getStationWeights(X, y, predictions, segments, N=12):\n",
      "    segmentWeights = []\n",
      "    for segment in range(N):\n",
      "        segmentIndex = np.argwhere(segments==(segment+1))[:,0]\n",
      "        segmentPredictions = predictions[:,segmentIndex]\n",
      "        segmentTruth = y[segmentIndex]\n",
      "        num_evaluations,best_score,best,first_score = hillclimb(segmentTruth, segmentPredictions, 11000)\n",
      "        print best_score\n",
      "        segmentWeights.append(best)\n",
      "    return segmentWeights\n",
      "\n",
      "def getAllWeights(stations, X, y, predictions, segments, N=12):\n",
      "    AllWeights = []\n",
      "    for i, station in enumerate(stations):\n",
      "        stationPredictions = predictions[i]\n",
      "        stationTruth = station.filterGEFSy(y)\n",
      "        weights = getStationWeights(X, stationTruth, stationPredictions, trainMonths, N)\n",
      "        AllWeights.append(weights)\n",
      "        print weights\n",
      "    return AllWeights\n",
      "\n",
      "def applyStationWeights(predictions, segmentWeights, segments, N=12):\n",
      "    for segment in range(N):\n",
      "        segmentIndex = np.argwhere(segments==(segment+1))[:,0]\n",
      "        predictions[:,segmentIndex] = (predictions[:,segmentIndex].T*np.array(segmentWeights[segment])).T\n",
      "        prediction = np.sum(predictions,0)\n",
      "    return prediction\n",
      "\n",
      "def applyAllWeights(stations, predictions, AllWeights, segments, N=12):\n",
      "    FinalPredictions = []\n",
      "    for i, station in enumerate(stations):\n",
      "        stationPredictions = np.copy(predictions[i])\n",
      "        segmentWeights = AllWeights[i]\n",
      "        prediction = applyStationWeights(stationPredictions, segmentWeights, segments, N)\n",
      "        FinalPredictions.append(prediction)\n",
      "    FinalPredictions = np.swapaxes(np.array(FinalPredictions),0,1)\n",
      "    return FinalPredictions"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 196
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "AllWeights = getAllWeights(stations[0:1], trainX, trainY, predictions, trainMonths, N=12)\n",
      "\n",
      "FinalPredictions =  applyAllWeights(stations[0:1], predictions, AllWeights, trainMonths, N=12)\n",
      "\n",
      "print metrics.mean_absolute_error(trainY[:,0],FinalPredictions[:,0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6813166890.88\n",
        "45688743.7294"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3.01513442984e+12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5.49272126638e+32"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "42413310763.9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2.083129243e+26"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2.88175974498e+15"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1.78865334488e+16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "197022642292.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "107128526.53"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "297418317.271"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1.10567466159e+40"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[22.092040816327188, 0.061224489795918366, 0.16326530612244897, -21.928775510204712, 0.16326530612244897, 0.10204081632653061, 0.18367346938775511, 0.040816326530612242, 0.061224489795918366, 0.061224489795918366, 0.0], [22.170000000000666, 0.050000000000000031, 0.17999999999999999, -21.980000000000636, 0.040000000000000001, 0.12, 0.059999999999999998, 0.12, 0.02, 0.040000000000000001, 0.17999999999999999], [0.27981132075471704, -0.045660377358490559, 2.7566037735848909, 0.13207547169811321, -2.5567924528301784, 0.11320754716981132, 0.056603773584905662, 0.018867924528301886, 0.11320754716981132, 0.018867924528301886, 0.11320754716981132], [4.3342857142856666, -4.191428571428526, 0.11428571428571428, 0.10000000000000001, 0.0, 0.085714285714285715, 0.085714285714285715, 0.10000000000000001, 0.12857142857142856, 0.11428571428571428, 0.12857142857142856], [-54.844545454543116, 55.099090909088517, 0.090909090909090912, 0.16363636363636364, 0.090909090909090912, 0.12727272727272726, 0.018181818181818181, 0.10909090909090909, 0.054545454545454543, 0.054545454545454543, 0.036363636363636362], [-0.035517241379310352, 4.8744827586206299, 0.068965517241379309, -4.5975862068964979, 0.034482758620689655, 0.0, 0.068965517241379309, 0.0, 0.2413793103448276, 0.13793103448275862, 0.20689655172413793], [-0.0025000000000000334, 5.4416666666665954, -5.1058333333332691, 0.0625, 0.0625, 0.10416666666666667, 0.083333333333333329, 0.0, 0.020833333333333332, 0.14583333333333334, 0.1875], [-27.163684210527766, -0.0031578947368421963, 27.508947368422554, 0.078947368421052627, 0.026315789473684209, 0.10526315789473684, 0.026315789473684209, 0.23684210526315788, 0.026315789473684209, 0.15789473684210525, 0.0], [-18.114545454545489, -0.0036363636363636355, 0.0, 18.436363636363719, 0.015151515151515152, 0.12121212121212122, 0.12121212121212122, 0.07575757575757576, 0.13636363636363635, 0.07575757575757576, 0.13636363636363635], [0.2466666666666667, -0.015833333333333317, 0.020833333333333332, 0.041666666666666664, -1.2350000000000008, 0.14583333333333334, 0.020833333333333332, 0.125, 0.041666666666666664, 1.5666666666666678, 0.041666666666666664], [-52.941304347824122, 53.27521739130232, 0.17391304347826086, -0.051304347826086956, 0.065217391304347824, 0.043478260869565216, 0.043478260869565216, 0.17391304347826086, 0.086956521739130432, 0.021739130434782608, 0.10869565217391304], [0.0, 0.0032432432432431858, -2.3429729729729671, 2.6099999999999883, 0.1891891891891892, 0.081081081081081086, 0.13513513513513514, 0.027027027027027029, 0.0, 0.1891891891891892, 0.10810810810810811]]\n",
        "9.38515208687e+38\n"
       ]
      }
     ],
     "prompt_number": 198
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'predictions', predictions.shape\n",
      "print 'trainY', trainY.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "predictions (98, 11, 5113)\n",
        "trainY (5113, 98)\n"
       ]
      }
     ],
     "prompt_number": 199
    }
   ],
   "metadata": {}
  }
 ]
}